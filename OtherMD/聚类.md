# 10种聚类算法

## 一、亲和力传播(Affinity Propagation)

##### 特点：

1. 不需要确定簇的个数
2. 已有的数据选为最终的聚类中心，而不是新生成一个簇中心
3. 模型对初始值不敏感；对初始相似矩阵的对称性没要求

<img src="https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/2751f0e9147da5b3acc5be252e103325.png" alt="img" style="zoom:150%;" />

##### Responsibility: 

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/ae4163777b68937ee7570699b5e442e1.png)

$$r(i,k)$$表示样本k作为样本i簇中心的程度

##### Availability:

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/2c5b955ed5a4cfbe6ec16ce81ebe124f.png)

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/97e8d0dacceca1691cd36f11afd1db93.png)

$$a(i,k)$$表示样本i对于样本k作为簇中心的支持；也可以表述为样本k适合做除了样本i外其他点簇中心的程度的相反数

##### Similarity：

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/f61ccbbdf2c6048979e5022fa7d41621.png)

样本点i和样本点k的距离，定义为欧氏距离的相反数。

<img src="https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/833c2060857aaf683abaa7056392b198.png" alt="img" style="zoom:150%;" />

如上图所示，对角线原本代表自己到自己的距离，但为了更新，可以定义成相似度的均值或中位数，最小值也可。

##### 具体代码关键步骤：

```python
r(i,k)new = λ*r(i,k)old + (1-λ)*r(i,k)
a(i,k)new = λ*a(i,k)old + (1-λ)*a(i,k)
```

AP算法通过迭代的方式来达到聚类效果，每次迭代其实就是更新上述两个矩阵的值, 在更新的时候，引入了一个叫做dumping factor的参数来控制更新的幅度。

##### 补充：

![image-20230122201111319](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/image-20230122201111319.png)

将聚类过程看成选举：

- 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表
- s(i,k)就相当于i对选k这个人的一个固有的偏好程度
- r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度
- r(i,k)的更新过程对应选民i的各个参选人的挑选（越出众越有吸引力）
- a(i,k)：从公式里可以看到，所有r(i',k)>0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i'们）都觉得k不错（r(i',k)>0），那么选民i也就会相应地觉得k不错，是个可以相信的选择
- a(i,k)的更新过程对应关于参选人k的[民意调查](https://www.zhihu.com/search?q=民意调查&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A47636054})对于选民i的影响（已经有了很多跟随者的人更有吸引力）
- 两者交替的过程也就可以理解为选民在各个参选人之间不断地比较和不断地参考各个参选人给出的民意调查。
- r(i,k)的思想反映的是竞争，a(i,k)则是为了让聚类更成功。

## 二、聚和聚类

输入：n个样本组成的样本集合及样本之间的距离；
输出：对样本集合的一个层次化聚类。

（1）计算n个样本两两之间的欧氏距离$$d_i$$，记作矩阵$$D=[d_{ij}]_{n\times n}$$

（2）构造n个类，每个类只包含一个样本。

（3）合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类。

（4）计算新类与当前各类的距离。若类的个数为1，终止计算，否则回到步（3）。

具体流程如下图所示：

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/20210418200343511.png)

## 三、综合层次聚类算法（BIRCH）

![img](https://miro.medium.com/max/1050/1*ArzxNmmLlzT8R2Tcua2w9Q.jpeg)

BIRCH首先针对大型数据集生成一些小而紧的摘要，然后对于这些摘要可以再使用其他聚类方法进行聚合。

BIRCH算法包括两个阶段：

1. **构建CF树：**BIRCH 将大型数据集汇总为较小的密集区域，称为聚类特征 (CF) 条目。形式上，聚类特征条目定义为有序三元组，(N, LS, SS)，其中“N”是聚类中数据点的数量，“LS”是数据点的线性和，“SS”是簇中数据点的平方和。一个 CF 条目可以由其他 CF 条目组成。可选地，我们可以将这个初始 CF 树压缩成一个更小的 CF。
2. **全局聚类：**在 CF 树的叶子上应用现有的聚类算法。CF 树是每个叶节点都包含一个子簇的树。CF 树中的每个条目都包含一个指向子节点的指针和一个由子节点中的 CF 条目之和组成的 CF 条目。或者，我们可以优化这些集群。

## 四、DBSCAN

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/20200726172343293.png)

##### 2个算法参数：

邻域半径R、最少点数目minpoints（MinPts）。

这两个算法参数实际可以刻画什么叫密集——当邻域半径R内的点的个数大于最少点数目minpoints时，就是密集。

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS84MC92Mi0wNDIzM2NhYmJiNzY4MmY2Mjk0YjliYmJhOGJiYzcwZF83MjB3LmpwZw)

##### 3个类别的点：

邻域半径R内样本点的数量大于等于minpoints的点叫做核心点。

不属于核心点但在某个核心点的邻域内的点叫做边界点。

既不是核心点也不是边界点的是噪声点。

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/aHR0cHM6Ly9waWNiLnpoaW1nLmNvbS92Mi0yZDY0NDJjYmY5ZjY5M2U4ZTIxZjUzN2EzNDliMzI4Zl9yLmpwZw)

##### 4个点的关系：

如果P为核心点，Q在P的R邻域内，那么称P到Q密度直达。任何核心点到其自身密度直达，密度直达不具有对称性，如果P到Q密度直达，那么Q到P不一定密度直达。

如果存在核心点P2，P3，……，Pn，且P1到P2密度直达，P2到P3密度直达，……，P(n-1)到Pn密度直达，Pn到Q密度直达，则P1到Q密度可达。密度可达也不具有对称性。

如果存在核心点S，使得S到P和Q都密度可达，则P和Q密度相连。密度相连具有对称性，如果P和Q密度相连，那么Q和P也一定密度相连。密度相连的两个点属于同一个聚类簇。

如果两个点不属于密度相连关系，则两个点非密度相连。非密度相连的两个点属于不同的聚类簇，或者其中存在噪声点。

![img](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS92Mi0xOTAwMTUxMGJiMzc2YTUzZmZhMDFhM2RiMmNhNGZkYl9yLmpwZw)

##### 总结

算法就是通过定义了密度可达的概念，把所有密度可达的点作为一个簇，密度可达具有传递性和对称性。

## 五、K均值（K-means）

![image-20230123164932317](https://cdn.jsdelivr.net/gh/JfyhDcm/TC/img/image-20230123164932317.png)

首先需要设定种类的个数，然后随机选择几个点代表不同的类，然后不断迭代簇类的中心，最终找到所有的类。















